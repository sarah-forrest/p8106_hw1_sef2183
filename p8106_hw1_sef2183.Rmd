---
title: "P8106 Data Science II Homework 1: Predicting the Sale Price of a House Using Characteristics"
author: "Sarah Forrest - sef2183"
date: "2/22/2023"
output: github_document
---

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(glmnet)
library(caret)
library(plotmo)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Data

In this exercise, we predict the sale price of a house using its other characteristics. The
training data are in “housing train.csv”, and the test data are in “housing test.csv”.

```{r}
# read in test data
test_data = read.csv("data/housing_test.csv") 
# read in training data
train_data = read.csv("data/housing_training.csv")
```

Create six input objects for use in the proceeding code:

1. x = input matrix of predictors x for the training data using `model.matrix()`
2. y = vector of response y for the training data

3. x2 = input matrix of predictors x for the test data using `model.matrix()`
4. y2 = vector of response y for the test data

5. ctrl1 = 10-fold cross-validation repeated 5 times using `trainControl()` and the default "best" minimum MSE rule
6. ctrl_1se = 10-fold cross-validation repeated 5 times using `trainControl()` and the 1 standard error (1SE) rule

```{r}
set.seed(1)

# training data
x <- model.matrix(Sale_Price ~ ., train_data)[ ,-1]
y <- train_data$Sale_Price

# test data
x2 <- model.matrix(Sale_Price ~ ., test_data)[ ,-1]
y2 <- test_data$Sale_Price

# 10-fold cross-validation repeated 5 times 
## best rule
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
## 1SE rule
ctrl_1se <- trainControl(method = "repeatedcv", selection = "oneSE", number = 10, repeats = 5)
```

# (a) Fit a linear model using least squares on the training data

Using the caret package: 

```{r}
set.seed(1)

lm_fit <- train(x, y,
                method = "lm",
                trControl = ctrl1)
```


# (b) Fit a lasso model on the training data 

Using the caret package: 

```{r}
set.seed(1)

lasso_fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-1, 5, length = 100))),
                   trControl = ctrl1)
```

## (bi) Tuning parameter (lambda)

```{r}
lasso_fit$bestTune
```

The best lambda value resulting in the minimum mean-square error (MSE) according to the caret package is 63.53019.

## (bii) Test error

```{r}
set.seed(1)

lasso_fit_pred <- predict(lasso_fit, newdata = x2)

# calculate MSE
mean((lasso_fit_pred - y2)^2)
# calculate RMSE
sqrt(mean((lasso_fit_pred - y2)^2)) 
```

The mean-square test error (MSE) for the lasso model is 440,154,088.
The root-mean-square test error (RMSE) for the lasso model is 20,979.85.

## (biii) Predictors

```{r}
set.seed(1)

lasso_fit_1se <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-1, 5, length = 100))),
                   trControl = ctrl_1se)

coef(lasso_fit_1se$finalModel, lasso_fit_1se$bestTune$lambda)
```

When the 1SE rule is applied, there are 36 predictors included in the model (not including the intercept). The following coefficients were removed from the model because they had values of 0: second floor square feet (Second_Flr_SF), "good" fireplace quality (Fireplace_QuNo_Fireplace), and no fireplace (Fireplace_QuNo_Fireplace )


# (c) Fit an elastic net model on the training data

Using the caret package:

```{r}
set.seed(1)

enet_fit <- train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(2, -2, length = 50))),
                  trControl = ctrl1)
```

## (bi) Tuning parameters (alpha and lambda)

```{r}
enet_fit$bestTune
```

The best alpha value using the minimum MSE rule according to the caret package is 0.65.
The best lambda value using the minimum MSE rule according to the caret package is 7.389056.

```{r}
set.seed(1)

enet_fit_1se <- train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(2, -2, length = 50))),
                  trControl = ctrl_1se)

enet_fit_1se$bestTune
```

It is possible to apply the 1SE rule to select the tuning parameters by adding a selection = "oneSE" statement to the `trainControl` function. When the 1SE rule is applied, the 
The best alpha value using the 1SE rule is 0.
The best lambda value using the 1SE rule is 7.389056.

## (bii) Test error

```{r}
set.seed(1)

enet_fit_pred <- predict(enet_fit, newdata = x2)

# calculate MSE
mean((enet_fit_pred - y2)^2)
# calculate RMSE
sqrt(mean((enet_fit_pred - y2)^2)) 
```

The mean-square test error (MSE) for the elastic net model is 442,815,907.
The root-mean-square test error (RMSE) for the elastic net model is 21,043.19.


# (d) Fit a partial least squares model on the training data

```{r}
set.seed(1)

pls_fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:19), trControl = ctrl1,
                 preProcess = c("center", "scale"))
```

## (bi) Test error 

```{r}
set.seed(1)

pls_fit_pred <- predict(pls_fit, newdata = x2)

# calculate MSE
mean((pls_fit_pred - y2)^2)
# calculate RMSE
sqrt(mean((pls_fit_pred - y2)^2)) 
```

The mean-square test error (MSE) for the partial least squares model is 449,622,718.
The root-mean-square test error (RMSE) for the partial least squares model is 21,204.31.

## (bii) Model components

```{r}
ggplot(pls_fit, highlight = TRUE)
```

There are 12 components included in the model. 


# (e) Model Comparison

```{r}
set.seed(1)

resamp <- resamples(list(
  pls = pls_fit,
  enet = enet_fit, 
  lasso = lasso_fit,
  lm = lm_fit)) 

summary(resamp)

bwplot(resamp, metric = "RMSE")
```

The best model that I would choose for predicting the sale price of a house is the lasso model. The reason for choosing this model is because it has the lowest mean value for RMSE.